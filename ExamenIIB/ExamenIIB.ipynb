{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Recuperacion de la Información - Examen IIB \n",
        "---\n",
        "**Nombre:** Anthony Reinoso\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RcsALepgWWfs",
        "outputId": "366805ca-1fcc-4288-8b8f-b2ab5de36b75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: rank_bm25 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.2.2)\n",
            "Requirement already satisfied: sentence-transformers in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (5.0.0)\n",
            "Requirement already satisfied: faiss-cpu in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.11.0.post1)\n",
            "Requirement already satisfied: transformers in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.53.3)\n",
            "Requirement already satisfied: numpy in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rank_bm25) (2.2.2)\n",
            "Requirement already satisfied: tqdm in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (2.7.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (0.33.4)\n",
            "Requirement already satisfied: Pillow in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.7.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Requirement already satisfied: setuptools in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\pituf\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\pituf\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "! pip install rank_bm25 sentence-transformers faiss-cpu transformers\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El archivo contiene 2792339 documentos.\n"
          ]
        }
      ],
      "source": [
        "def contar_documentos(path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        total = sum(1 for _ in f)\n",
        "    return total\n",
        "\n",
        "ruta = 'arxiv-metadata-oai-snapshot.json'\n",
        "total_docs = contar_documentos(ruta)\n",
        "print(f\"El archivo contiene {total_docs} documentos.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implementación de la arquitectura."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEJVYKyBYYVQ"
      },
      "source": [
        "## 1. Preprocesamiento del corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eDMl6rdYVXA"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import string\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words = set(stopwords.words('english')) # Configuración de stopword\n",
        "\n",
        "# Función de preprocesamiento\n",
        "def preprocess(text):\n",
        "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def load_corpus(path, max_docs=28000):\n",
        "    corpus = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= max_docs:\n",
        "                break\n",
        "            doc = json.loads(line)\n",
        "            doc['text'] = preprocess(doc['title'] + ' ' + doc['abstract'])\n",
        "            corpus.append(doc)\n",
        "    return corpus\n",
        "\n",
        "corpus = load_corpus('arxiv-metadata-oai-snapshot.json')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mostrar en tabla el corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Texto original</th>\n",
              "      <th>Texto preprocesado</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Calculation of prompt diphoton production cros...</td>\n",
              "      <td>calculation prompt diphoton production cross s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sparsity-certifying Graph Decompositions   We ...</td>\n",
              "      <td>sparsitycertifying graph decompositions descri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The evolution of the Earth-Moon system based o...</td>\n",
              "      <td>evolution earthmoon system based dark matter f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
              "      <td>determinant stirling cycle numbers counts unla...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
              "      <td>dyadic lambdaalpha lambdaalpha paper show comp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Bosonic characters of atomic Cooper pairs acro...</td>\n",
              "      <td>bosonic characters atomic cooper pairs across ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Polymer Quantum Mechanics and its Continuum Li...</td>\n",
              "      <td>polymer quantum mechanics continuum limit rath...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Numerical solution of shock and ramp compressi...</td>\n",
              "      <td>numerical solution shock ramp compression gene...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>The Spitzer c2d Survey of Large, Nearby, Inste...</td>\n",
              "      <td>spitzer c2d survey large nearby insterstellar ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Partial cubes: structures, characterizations, ...</td>\n",
              "      <td>partial cubes structures characterizations con...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      Texto original  \\\n",
              "0  Calculation of prompt diphoton production cros...   \n",
              "1  Sparsity-certifying Graph Decompositions   We ...   \n",
              "2  The evolution of the Earth-Moon system based o...   \n",
              "3  A determinant of Stirling cycle numbers counts...   \n",
              "4  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n",
              "5  Bosonic characters of atomic Cooper pairs acro...   \n",
              "6  Polymer Quantum Mechanics and its Continuum Li...   \n",
              "7  Numerical solution of shock and ramp compressi...   \n",
              "8  The Spitzer c2d Survey of Large, Nearby, Inste...   \n",
              "9  Partial cubes: structures, characterizations, ...   \n",
              "\n",
              "                                  Texto preprocesado  \n",
              "0  calculation prompt diphoton production cross s...  \n",
              "1  sparsitycertifying graph decompositions descri...  \n",
              "2  evolution earthmoon system based dark matter f...  \n",
              "3  determinant stirling cycle numbers counts unla...  \n",
              "4  dyadic lambdaalpha lambdaalpha paper show comp...  \n",
              "5  bosonic characters atomic cooper pairs across ...  \n",
              "6  polymer quantum mechanics continuum limit rath...  \n",
              "7  numerical solution shock ramp compression gene...  \n",
              "8  spitzer c2d survey large nearby insterstellar ...  \n",
              "9  partial cubes structures characterizations con...  "
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Carga de corpus con textos originales y preprocesados\n",
        "def load_corpus_for_display(path, max_docs=10):\n",
        "    original = []\n",
        "    procesado = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= max_docs:\n",
        "                break\n",
        "            doc = json.loads(line)\n",
        "            title = doc.get('title', '')\n",
        "            abstract = doc.get('abstract', '')\n",
        "            combined_text = title + ' ' + abstract\n",
        "            original.append(combined_text)\n",
        "            procesado.append(preprocess(combined_text))\n",
        "    return pd.DataFrame({\n",
        "        \"Texto original\": original,\n",
        "        \"Texto preprocesado\": procesado\n",
        "    })\n",
        "\n",
        "df_corpus_display = load_corpus_for_display(\"arxiv-metadata-oai-snapshot.json\", max_docs=10)\n",
        "df_corpus_display.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjfgIWJ7Ydia"
      },
      "source": [
        "## 2. Indexación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsjleJ_HYiep"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aNlfxTVxYVUx"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform([doc['text'] for doc in corpus])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5jdq95IYnWT"
      },
      "source": [
        "### BM25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cAuHn4PMYVSY"
      },
      "outputs": [],
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "tokenized_corpus = [doc['text'].split() for doc in corpus]\n",
        "bm25 = BM25Okapi(tokenized_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUaAQ851Yqkq"
      },
      "source": [
        "### Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7qaXqixnYVQP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\pituf\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Batches: 100%|██████████| 875/875 [12:22<00:00,  1.18it/s] \n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embeddings = model.encode([doc['text'] for doc in corpus], show_progress_bar=True)\n",
        "index_faiss = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index_faiss.add(np.array(embeddings))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Recuperacion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PHrOzvFpYVJw"
      },
      "outputs": [],
      "source": [
        "def search_tfidf(query, top_k=10):\n",
        "    query_vec = tfidf_vectorizer.transform([preprocess(query)])\n",
        "    scores = tfidf_matrix.dot(query_vec.T).toarray().ravel()\n",
        "    top_indices = scores.argsort()[::-1][:top_k]\n",
        "    return [corpus[i] for i in top_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### BM25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def search_bm25(query, top_k=10):\n",
        "    tokens = preprocess(query).split()\n",
        "    scores = bm25.get_scores(tokens)\n",
        "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
        "    return [corpus[i] for i in top_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def search_faiss(query, top_k=10):\n",
        "    vec = model.encode([preprocess(query)])\n",
        "    scores, indices = index_faiss.search(np.array(vec), top_k)\n",
        "    return [corpus[i] for i in indices[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\pituf\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pituf\\.cache\\huggingface\\hub\\models--google--flan-t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "rag_pipeline = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
        "\n",
        "def generate_rag_response(query, top_k=3):\n",
        "    top_docs = search_faiss(query, top_k)\n",
        "    context = \" \".join([doc['abstract'] for doc in top_docs])\n",
        "    prompt = f\"Query: {query}\\nContext: {context}\\nAnswer:\"\n",
        "    response = rag_pipeline(prompt, max_length=256)[0]['generated_text']\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparación de resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coincidencias entre modelos:\n",
            " {'TF-IDF - BM25': ['0704.3905', '0704.3453', '0708.1564'], 'TF-IDF - FAISS': [], 'BM25 - FAISS': ['0707.0930'], 'Todos en común': []}\n"
          ]
        }
      ],
      "source": [
        "def compare_top10_ids(query):\n",
        "    tfidf_ids = [doc['id'] for doc in search_tfidf(query)]\n",
        "    bm25_ids = [doc['id'] for doc in search_bm25(query)]\n",
        "    faiss_ids = [doc['id'] for doc in search_faiss(query)]\n",
        "\n",
        "    common_ids = {\n",
        "        'TF-IDF - BM25': list(set(tfidf_ids) & set(bm25_ids)),\n",
        "        'TF-IDF - FAISS': list(set(tfidf_ids) & set(faiss_ids)),\n",
        "        'BM25 - FAISS': list(set(bm25_ids) & set(faiss_ids)),\n",
        "        'Todos en común': list(set(tfidf_ids) & set(bm25_ids) & set(faiss_ids))\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        'TF-IDF': tfidf_ids,\n",
        "        'BM25': bm25_ids,\n",
        "        'FAISS': faiss_ids,\n",
        "        'Coincidencias': common_ids\n",
        "    }\n",
        "\n",
        "query = \"machine learning for particle physics\"\n",
        "resultados = compare_top10_ids(query)\n",
        "print(\"Coincidencias entre modelos:\\n\", resultados['Coincidencias'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **¿Cuáles documentos aparecen en común?**\n",
        "Al comparar los documentos recuperados por los tres modelos para una misma consulta, se encontró lo siguiente:\n",
        "\n",
        "- TF-IDF y BM25 comparten 3 documentos en común: 0704.3905, 0704.3453 y 0708.1564. Esto muestra que ambos modelos, al basarse en estadísticas de frecuencia de palabras, tienden a coincidir cuando se usan términos específicos en la búsqueda.\n",
        "\n",
        "- BM25 y FAISS solo comparten 1 documento (0707.0930), lo cual sugiere que FAISS recupera resultados con un enfoque más semántico, mientras que BM25 sigue más fiel a las coincidencias de términos.\n",
        "\n",
        "- TF-IDF y FAISS no tienen documentos en común, lo que indica una diferencia clara en cómo interpretan la consulta.\n",
        "\n",
        "- No hubo ningún documento que aparezca en los tres modelos a la vez, lo que refuerza la idea de que cada modelo prioriza distintos aspectos del contenido.\n",
        "\n",
        "#### **¿Qué diferencias hay en el ordenamiento?**\n",
        "- TF-IDF y BM25 tienden a priorizar los documentos donde las palabras clave aparecen con más frecuencia o en posiciones relevantes dentro del texto.\n",
        "\n",
        "- FAISS, al usar embeddings, ordena los resultados según el significado general del texto, aunque no contenga las palabras exactas. Esto hace que el orden de resultados sea completamente diferente a los modelos clásicos.\n",
        "\n",
        "Por eso, aunque dos artículos traten el mismo tema, si usan diferentes palabras o sinónimos, FAISS los puede ubicar más arriba que TF-IDF o BM25."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Medir similitud entre ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de coincidencias (top 10):\n",
            "{'TF-IDF - BM25': 3, 'TF-IDF - FAISS': 0, 'BM25 - FAISS': 1, 'Todos': 0}\n"
          ]
        }
      ],
      "source": [
        "def count_common_documents(query):\n",
        "    tfidf_ids = set([doc['id'] for doc in search_tfidf(query)])\n",
        "    bm25_ids = set([doc['id'] for doc in search_bm25(query)])\n",
        "    faiss_ids = set([doc['id'] for doc in search_faiss(query)])\n",
        "\n",
        "    inter_tfidf_bm25 = tfidf_ids & bm25_ids\n",
        "    inter_tfidf_faiss = tfidf_ids & faiss_ids\n",
        "    inter_bm25_faiss = bm25_ids & faiss_ids\n",
        "\n",
        "    all_common = tfidf_ids & bm25_ids & faiss_ids\n",
        "\n",
        "    return {\n",
        "        \"TF-IDF - BM25\": len(inter_tfidf_bm25),\n",
        "        \"TF-IDF - FAISS\": len(inter_tfidf_faiss),\n",
        "        \"BM25 - FAISS\": len(inter_bm25_faiss),\n",
        "        \"Todos\": len(all_common)\n",
        "    }\n",
        "\n",
        "print(\"Cantidad de coincidencias (top 10):\")\n",
        "print(count_common_documents(query))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Títulos de documentos recuperados:\n",
            "1. A threshold-improved narrow-width approximation for BSM physics\n",
            "2. What can we learn from fluctuations of particle ratios?\n",
            "3. Bayesian Learning of Neural Networks for Signal/Background\n",
            "  Discrimination in Particle Physics\n",
            "\n",
            "Resumen combinado (contexto):\n",
            "-    A modified narrow-width approximation that allows for O(Gamma/M)-accurate\n",
            "predictions for resonant particle decay with similar intermediate masses is\n",
            "proposed and applied to MSSM processes to demonstrate its importance for\n",
            "searches for particle physics beyond the Standard Model.\n",
            " ...\n",
            "\n",
            "-    We explain how fluctuations of ratios can constrain and falsify the\n",
            "statistical model of particle production in heavy ion collisions, using $K/\\pi$\n",
            "fluctuations as an example. We define an observable capable of determining\n",
            "which statistical model, if any, governs freeze-out in ultrarelativistic he ...\n",
            "\n",
            "-    Neural networks are used extensively in classification problems in particle\n",
            "physics research. Since the training of neural networks can be viewed as a\n",
            "problem of inference, Bayesian learning of neural networks can provide more\n",
            "optimal and robust results than conventional learning methods. We have\n",
            " ...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Respuesta generada por RAG:\n",
            " A modified narrow-width approximation that allows for O(Gamma/M)-accurate predictions for resonant particle decay with similar intermediate masses is proposed and applied to MSSM processes to demonstrate its importance for searches for particle physics beyond the Standard Model. We explain how fluctuations of ratios can constrain and falsify the statistical model of particle production in heavy ion collisions, using $K/pi$ fluctuations as an example. We define an observable capable of determining which statistical model, if any, governs freeze-out in ultrarelativistic heavy ion collisions, using $K/pi$ fluctuations as an example. We calculate this observable for $K/pi$ fluctuations, and show that it should be the same for RHIC and LHC energies, as well as independent of centrality, if the Grand-Canonical statistical model is an appropriate description and chemical equilibrium applies. We also introduce a similar observable capable, together with the published $K*/K$ measurement, of ascertaining if an interacting hadron gas phase governs the system between thermal and\n"
          ]
        }
      ],
      "source": [
        "def show_rag_context_and_answer(query):\n",
        "    top_docs = search_faiss(query, top_k=3)\n",
        "    print(\"Títulos de documentos recuperados:\")\n",
        "    for i, doc in enumerate(top_docs):\n",
        "        print(f\"{i+1}. {doc['title']}\")\n",
        "    \n",
        "    print(\"\\nResumen combinado (contexto):\")\n",
        "    for doc in top_docs:\n",
        "        print(\"- \", doc['abstract'][:300], \"...\\n\")\n",
        "\n",
        "    respuesta = generate_rag_response(query)\n",
        "    print(\"\\n Respuesta generada por RAG:\\n\", respuesta)\n",
        "\n",
        "show_rag_context_and_answer(\"machine learning for particle physics\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Tabla comparativa (Benchmark) de resultados entre modelos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calidad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Query</th>\n",
              "      <th>TF-IDF - BM25</th>\n",
              "      <th>TF-IDF - FAISS</th>\n",
              "      <th>BM25 - FAISS</th>\n",
              "      <th>Todos comunes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>diphoton production cross sections</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>quantum chromodynamics</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>higgs boson decay</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>machine learning for particle physics</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>top quark production</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   Query  TF-IDF - BM25  TF-IDF - FAISS  \\\n",
              "0     diphoton production cross sections              9               4   \n",
              "1                 quantum chromodynamics              8               4   \n",
              "2                      higgs boson decay              8               2   \n",
              "3  machine learning for particle physics              3               0   \n",
              "4                   top quark production              5               5   \n",
              "\n",
              "   BM25 - FAISS  Todos comunes  \n",
              "0             3              3  \n",
              "1             4              4  \n",
              "2             4              2  \n",
              "3             1              0  \n",
              "4             5              3  "
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "# Leer queries desde el archivo txt\n",
        "with open(\"queries.txt\", \"r\") as f:\n",
        "    queries = [line.strip() for line in f.readlines() if line.strip()]\n",
        "\n",
        "# Función para evaluar calidad de recuperación\n",
        "def benchmark_models(queries):\n",
        "    records = []\n",
        "    for query in queries:\n",
        "        tfidf_ids = set([doc[\"id\"] for doc in search_tfidf(query)])\n",
        "        bm25_ids = set([doc[\"id\"] for doc in search_bm25(query)])\n",
        "        faiss_ids = set([doc[\"id\"] for doc in search_faiss(query)])\n",
        "\n",
        "        records.append({\n",
        "            \"Query\": query,\n",
        "            \"TF-IDF - BM25\": len(tfidf_ids & bm25_ids),\n",
        "            \"TF-IDF - FAISS\": len(tfidf_ids & faiss_ids),\n",
        "            \"BM25 - FAISS\": len(bm25_ids & faiss_ids),\n",
        "            \"Todos comunes\": len(tfidf_ids & bm25_ids & faiss_ids)\n",
        "        })\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "# Ejecutar el benchmark y mostrar la tabla\n",
        "df_benchmark = benchmark_models(queries)\n",
        "df_benchmark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La tabla nos dice qué tan parecidos son los resultados que devuelve cada modelo.\n",
        "Vemos que TF-IDF y BM25 suelen coincidir bastante, porque ambos buscan las palabras tal como las escribiste.\n",
        "FAISS, en cambio, busca más por el sentido o el significado de lo que preguntas, así que encuentra cosas distintas.\n",
        "Por eso, casi no hay documentos que aparezcan en los tres modelos a la vez. Cada uno entiende la búsqueda a su manera.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ejemplo de una consulta y su respuesta generada con RAG\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Respuesta generada con RAG:\n",
            " A modified narrow-width approximation that allows for O(Gamma/M)-accurate predictions for resonant particle decay with similar intermediate masses is proposed and applied to MSSM processes to demonstrate its importance for searches for particle physics beyond the Standard Model. We explain how fluctuations of ratios can constrain and falsify the statistical model of particle production in heavy ion collisions, using $K/pi$ fluctuations as an example. We define an observable capable of determining which statistical model, if any, governs freeze-out in ultrarelativistic heavy ion collisions, using $K/pi$ fluctuations as an example. We calculate this observable for $K/pi$ fluctuations, and show that it should be the same for RHIC and LHC energies, as well as independent of centrality, if the Grand-Canonical statistical model is an appropriate description and chemical equilibrium applies. We also introduce a similar observable capable, together with the published $K*/K$ measurement, of ascertaining if an interacting hadron gas phase governs the system between thermal and\n"
          ]
        }
      ],
      "source": [
        "query = \"machine learning for particle physics\"\n",
        "respuesta = generate_rag_response(query)\n",
        "print(\"Respuesta generada con RAG:\\n\", respuesta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Diferencias entre modelos y utilidad del RAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Modelo</th>\n",
              "      <th>Ventajas</th>\n",
              "      <th>Limitaciones</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TF-IDF</td>\n",
              "      <td>Rápido, fácil de implementar y eficiente en bú...</td>\n",
              "      <td>No capta el significado de las palabras; no de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BM25</td>\n",
              "      <td>Mejor manejo de la frecuencia de términos y má...</td>\n",
              "      <td>Aunque mejora a TF-IDF, aún es un modelo estad...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>FAISS</td>\n",
              "      <td>Captura similitud semántica entre frases o sin...</td>\n",
              "      <td>Necesita mayor poder computacional y puede rec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RAG (con Flan-T5)</td>\n",
              "      <td>Genera respuestas completas y justificadas usa...</td>\n",
              "      <td>Depende de la calidad del índice vectorial y d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Modelo                                           Ventajas  \\\n",
              "0             TF-IDF  Rápido, fácil de implementar y eficiente en bú...   \n",
              "1               BM25  Mejor manejo de la frecuencia de términos y má...   \n",
              "2              FAISS  Captura similitud semántica entre frases o sin...   \n",
              "3  RAG (con Flan-T5)  Genera respuestas completas y justificadas usa...   \n",
              "\n",
              "                                        Limitaciones  \n",
              "0  No capta el significado de las palabras; no de...  \n",
              "1  Aunque mejora a TF-IDF, aún es un modelo estad...  \n",
              "2  Necesita mayor poder computacional y puede rec...  \n",
              "3  Depende de la calidad del índice vectorial y d...  "
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "comparacion_modelos = pd.DataFrame({\n",
        "    \"Modelo\": [\"TF-IDF\", \"BM25\", \"FAISS\", \"RAG (con Flan-T5)\"],\n",
        "    \"Ventajas\": [\n",
        "        \"Es rápido, fácil de programar y te devuelve buenos resultados si buscas palabras exactas.\",\n",
        "        \"Es más listo que TF-IDF porque da más peso a las palabras que realmente importan.\",\n",
        "        \"Captura similitud semántica entre frases o sinónimos gracias a embeddings.\",\n",
        "        \"Es como un asistente que busca y te explica. Resume la info y contesta con base en lo que encontró.\"\n",
        "    ],\n",
        "    \"Limitaciones\": [\n",
        "        \"No entiende el significado de las palabras. Si usas un sinónimo, no lo capta.\",\n",
        "        \"Aun así, sigue sin entender ideas, solo se enfoca en cuántas veces aparece algo.\",\n",
        "        \"Necesita mayor poder computacional y puede recuperar documentos no tan precisos.\",\n",
        "        \"Depende de la calidad del índice vectorial y del modelo generativo usado.\"\n",
        "    ]\n",
        "})\n",
        "comparacion_modelos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utilidad del RAG\n",
        "\n",
        "Es útil cuando no se tiene tiempo (por ciertas circuntancias) para leer todo o cuando los textos son muy técnicos. RAG da una respuesta clara, directa y con contexto, sin tener que estar adivinando qué dice cada paper.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
