{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Recuperacion de la InformaciÃ³n - Examen IIB \n",
        "---\n",
        "**Nombre:** Anthony Reinoso\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RcsALepgWWfs",
        "outputId": "366805ca-1fcc-4288-8b8f-b2ab5de36b75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: rank_bm25 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.2.2)\n",
            "Requirement already satisfied: sentence-transformers in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (5.0.0)\n",
            "Requirement already satisfied: faiss-cpu in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.11.0.post1)\n",
            "Requirement already satisfied: transformers in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.53.3)\n",
            "Requirement already satisfied: numpy in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rank_bm25) (2.2.2)\n",
            "Requirement already satisfied: tqdm in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (2.7.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (0.33.4)\n",
            "Requirement already satisfied: Pillow in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.7.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Requirement already satisfied: setuptools in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\pituf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\pituf\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\pituf\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "! pip install rank_bm25 sentence-transformers faiss-cpu transformers\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El archivo contiene 2792339 documentos.\n"
          ]
        }
      ],
      "source": [
        "def contar_documentos(path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        total = sum(1 for _ in f)\n",
        "    return total\n",
        "\n",
        "ruta = 'arxiv-metadata-oai-snapshot.json'\n",
        "total_docs = contar_documentos(ruta)\n",
        "print(f\"El archivo contiene {total_docs} documentos.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ImplementaciÃ³n de la arquitectura."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEJVYKyBYYVQ"
      },
      "source": [
        "## 1. Preprocesamiento del corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eDMl6rdYVXA"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import string\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words = set(stopwords.words('english')) # ConfiguraciÃ³n de stopword\n",
        "\n",
        "# FunciÃ³n de preprocesamiento\n",
        "def preprocess(text):\n",
        "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def load_corpus(path, max_docs=28000):\n",
        "    corpus = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= max_docs:\n",
        "                break\n",
        "            doc = json.loads(line)\n",
        "            doc['text'] = preprocess(doc['title'] + ' ' + doc['abstract'])\n",
        "            corpus.append(doc)\n",
        "    return corpus\n",
        "\n",
        "corpus = load_corpus('arxiv-metadata-oai-snapshot.json')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mostrar en tabla el corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Texto original</th>\n",
              "      <th>Texto preprocesado</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Calculation of prompt diphoton production cros...</td>\n",
              "      <td>calculation prompt diphoton production cross s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sparsity-certifying Graph Decompositions   We ...</td>\n",
              "      <td>sparsitycertifying graph decompositions descri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The evolution of the Earth-Moon system based o...</td>\n",
              "      <td>evolution earthmoon system based dark matter f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
              "      <td>determinant stirling cycle numbers counts unla...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
              "      <td>dyadic lambdaalpha lambdaalpha paper show comp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Bosonic characters of atomic Cooper pairs acro...</td>\n",
              "      <td>bosonic characters atomic cooper pairs across ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Polymer Quantum Mechanics and its Continuum Li...</td>\n",
              "      <td>polymer quantum mechanics continuum limit rath...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Numerical solution of shock and ramp compressi...</td>\n",
              "      <td>numerical solution shock ramp compression gene...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>The Spitzer c2d Survey of Large, Nearby, Inste...</td>\n",
              "      <td>spitzer c2d survey large nearby insterstellar ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Partial cubes: structures, characterizations, ...</td>\n",
              "      <td>partial cubes structures characterizations con...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      Texto original  \\\n",
              "0  Calculation of prompt diphoton production cros...   \n",
              "1  Sparsity-certifying Graph Decompositions   We ...   \n",
              "2  The evolution of the Earth-Moon system based o...   \n",
              "3  A determinant of Stirling cycle numbers counts...   \n",
              "4  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n",
              "5  Bosonic characters of atomic Cooper pairs acro...   \n",
              "6  Polymer Quantum Mechanics and its Continuum Li...   \n",
              "7  Numerical solution of shock and ramp compressi...   \n",
              "8  The Spitzer c2d Survey of Large, Nearby, Inste...   \n",
              "9  Partial cubes: structures, characterizations, ...   \n",
              "\n",
              "                                  Texto preprocesado  \n",
              "0  calculation prompt diphoton production cross s...  \n",
              "1  sparsitycertifying graph decompositions descri...  \n",
              "2  evolution earthmoon system based dark matter f...  \n",
              "3  determinant stirling cycle numbers counts unla...  \n",
              "4  dyadic lambdaalpha lambdaalpha paper show comp...  \n",
              "5  bosonic characters atomic cooper pairs across ...  \n",
              "6  polymer quantum mechanics continuum limit rath...  \n",
              "7  numerical solution shock ramp compression gene...  \n",
              "8  spitzer c2d survey large nearby insterstellar ...  \n",
              "9  partial cubes structures characterizations con...  "
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Carga de corpus con textos originales y preprocesados\n",
        "def load_corpus_for_display(path, max_docs=10):\n",
        "    original = []\n",
        "    procesado = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= max_docs:\n",
        "                break\n",
        "            doc = json.loads(line)\n",
        "            title = doc.get('title', '')\n",
        "            abstract = doc.get('abstract', '')\n",
        "            combined_text = title + ' ' + abstract\n",
        "            original.append(combined_text)\n",
        "            procesado.append(preprocess(combined_text))\n",
        "    return pd.DataFrame({\n",
        "        \"Texto original\": original,\n",
        "        \"Texto preprocesado\": procesado\n",
        "    })\n",
        "\n",
        "df_corpus_display = load_corpus_for_display(\"arxiv-metadata-oai-snapshot.json\", max_docs=10)\n",
        "df_corpus_display.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjfgIWJ7Ydia"
      },
      "source": [
        "## 2. IndexaciÃ³n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsjleJ_HYiep"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aNlfxTVxYVUx"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform([doc['text'] for doc in corpus])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5jdq95IYnWT"
      },
      "source": [
        "### BM25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cAuHn4PMYVSY"
      },
      "outputs": [],
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "tokenized_corpus = [doc['text'].split() for doc in corpus]\n",
        "bm25 = BM25Okapi(tokenized_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUaAQ851Yqkq"
      },
      "source": [
        "### Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7qaXqixnYVQP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\pituf\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Batches: 100%|ââââââââââ| 875/875 [12:22<00:00,  1.18it/s] \n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embeddings = model.encode([doc['text'] for doc in corpus], show_progress_bar=True)\n",
        "index_faiss = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index_faiss.add(np.array(embeddings))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Recuperacion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PHrOzvFpYVJw"
      },
      "outputs": [],
      "source": [
        "def search_tfidf(query, top_k=10):\n",
        "    query_vec = tfidf_vectorizer.transform([preprocess(query)])\n",
        "    scores = tfidf_matrix.dot(query_vec.T).toarray().ravel()\n",
        "    top_indices = scores.argsort()[::-1][:top_k]\n",
        "    return [corpus[i] for i in top_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### BM25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def search_bm25(query, top_k=10):\n",
        "    tokens = preprocess(query).split()\n",
        "    scores = bm25.get_scores(tokens)\n",
        "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
        "    return [corpus[i] for i in top_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def search_faiss(query, top_k=10):\n",
        "    vec = model.encode([preprocess(query)])\n",
        "    scores, indices = index_faiss.search(np.array(vec), top_k)\n",
        "    return [corpus[i] for i in indices[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\pituf\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pituf\\.cache\\huggingface\\hub\\models--google--flan-t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "rag_pipeline = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
        "\n",
        "def generate_rag_response(query, top_k=3):\n",
        "    top_docs = search_faiss(query, top_k)\n",
        "    context = \" \".join([doc['abstract'] for doc in top_docs])\n",
        "    prompt = f\"Query: {query}\\nContext: {context}\\nAnswer:\"\n",
        "    response = rag_pipeline(prompt, max_length=256)[0]['generated_text']\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. EvaluaciÃ³n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ComparaciÃ³n de resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coincidencias entre modelos:\n",
            " {'TF-IDF - BM25': ['0704.3905', '0704.3453', '0708.1564'], 'TF-IDF - FAISS': [], 'BM25 - FAISS': ['0707.0930'], 'Todos en comÃºn': []}\n"
          ]
        }
      ],
      "source": [
        "def compare_top10_ids(query):\n",
        "    tfidf_ids = [doc['id'] for doc in search_tfidf(query)]\n",
        "    bm25_ids = [doc['id'] for doc in search_bm25(query)]\n",
        "    faiss_ids = [doc['id'] for doc in search_faiss(query)]\n",
        "\n",
        "    common_ids = {\n",
        "        'TF-IDF - BM25': list(set(tfidf_ids) & set(bm25_ids)),\n",
        "        'TF-IDF - FAISS': list(set(tfidf_ids) & set(faiss_ids)),\n",
        "        'BM25 - FAISS': list(set(bm25_ids) & set(faiss_ids)),\n",
        "        'Todos en comÃºn': list(set(tfidf_ids) & set(bm25_ids) & set(faiss_ids))\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        'TF-IDF': tfidf_ids,\n",
        "        'BM25': bm25_ids,\n",
        "        'FAISS': faiss_ids,\n",
        "        'Coincidencias': common_ids\n",
        "    }\n",
        "\n",
        "query = \"machine learning for particle physics\"\n",
        "resultados = compare_top10_ids(query)\n",
        "print(\"Coincidencias entre modelos:\\n\", resultados['Coincidencias'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Â¿CuÃ¡les documentos aparecen en comÃºn?**\n",
        "Al comparar los documentos recuperados por los tres modelos para una misma consulta, se encontrÃ³ lo siguiente:\n",
        "\n",
        "- TF-IDF y BM25 comparten 3 documentos en comÃºn: 0704.3905, 0704.3453 y 0708.1564. Esto muestra que ambos modelos, al basarse en estadÃ­sticas de frecuencia de palabras, tienden a coincidir cuando se usan tÃ©rminos especÃ­ficos en la bÃºsqueda.\n",
        "\n",
        "- BM25 y FAISS solo comparten 1 documento (0707.0930), lo cual sugiere que FAISS recupera resultados con un enfoque mÃ¡s semÃ¡ntico, mientras que BM25 sigue mÃ¡s fiel a las coincidencias de tÃ©rminos.\n",
        "\n",
        "- TF-IDF y FAISS no tienen documentos en comÃºn, lo que indica una diferencia clara en cÃ³mo interpretan la consulta.\n",
        "\n",
        "- No hubo ningÃºn documento que aparezca en los tres modelos a la vez, lo que refuerza la idea de que cada modelo prioriza distintos aspectos del contenido.\n",
        "\n",
        "#### **Â¿QuÃ© diferencias hay en el ordenamiento?**\n",
        "- TF-IDF y BM25 tienden a priorizar los documentos donde las palabras clave aparecen con mÃ¡s frecuencia o en posiciones relevantes dentro del texto.\n",
        "\n",
        "- FAISS, al usar embeddings, ordena los resultados segÃºn el significado general del texto, aunque no contenga las palabras exactas. Esto hace que el orden de resultados sea completamente diferente a los modelos clÃ¡sicos.\n",
        "\n",
        "Por eso, aunque dos artÃ­culos traten el mismo tema, si usan diferentes palabras o sinÃ³nimos, FAISS los puede ubicar mÃ¡s arriba que TF-IDF o BM25."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Medir similitud entre ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de coincidencias (top 10):\n",
            "{'TF-IDF - BM25': 3, 'TF-IDF - FAISS': 0, 'BM25 - FAISS': 1, 'Todos': 0}\n"
          ]
        }
      ],
      "source": [
        "def count_common_documents(query):\n",
        "    tfidf_ids = set([doc['id'] for doc in search_tfidf(query)])\n",
        "    bm25_ids = set([doc['id'] for doc in search_bm25(query)])\n",
        "    faiss_ids = set([doc['id'] for doc in search_faiss(query)])\n",
        "\n",
        "    inter_tfidf_bm25 = tfidf_ids & bm25_ids\n",
        "    inter_tfidf_faiss = tfidf_ids & faiss_ids\n",
        "    inter_bm25_faiss = bm25_ids & faiss_ids\n",
        "\n",
        "    all_common = tfidf_ids & bm25_ids & faiss_ids\n",
        "\n",
        "    return {\n",
        "        \"TF-IDF - BM25\": len(inter_tfidf_bm25),\n",
        "        \"TF-IDF - FAISS\": len(inter_tfidf_faiss),\n",
        "        \"BM25 - FAISS\": len(inter_bm25_faiss),\n",
        "        \"Todos\": len(all_common)\n",
        "    }\n",
        "\n",
        "print(\"Cantidad de coincidencias (top 10):\")\n",
        "print(count_common_documents(query))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TÃ­tulos de documentos recuperados:\n",
            "1. A threshold-improved narrow-width approximation for BSM physics\n",
            "2. What can we learn from fluctuations of particle ratios?\n",
            "3. Bayesian Learning of Neural Networks for Signal/Background\n",
            "  Discrimination in Particle Physics\n",
            "\n",
            "Resumen combinado (contexto):\n",
            "-    A modified narrow-width approximation that allows for O(Gamma/M)-accurate\n",
            "predictions for resonant particle decay with similar intermediate masses is\n",
            "proposed and applied to MSSM processes to demonstrate its importance for\n",
            "searches for particle physics beyond the Standard Model.\n",
            " ...\n",
            "\n",
            "-    We explain how fluctuations of ratios can constrain and falsify the\n",
            "statistical model of particle production in heavy ion collisions, using $K/\\pi$\n",
            "fluctuations as an example. We define an observable capable of determining\n",
            "which statistical model, if any, governs freeze-out in ultrarelativistic he ...\n",
            "\n",
            "-    Neural networks are used extensively in classification problems in particle\n",
            "physics research. Since the training of neural networks can be viewed as a\n",
            "problem of inference, Bayesian learning of neural networks can provide more\n",
            "optimal and robust results than conventional learning methods. We have\n",
            " ...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Respuesta generada por RAG:\n",
            " A modified narrow-width approximation that allows for O(Gamma/M)-accurate predictions for resonant particle decay with similar intermediate masses is proposed and applied to MSSM processes to demonstrate its importance for searches for particle physics beyond the Standard Model. We explain how fluctuations of ratios can constrain and falsify the statistical model of particle production in heavy ion collisions, using $K/pi$ fluctuations as an example. We define an observable capable of determining which statistical model, if any, governs freeze-out in ultrarelativistic heavy ion collisions, using $K/pi$ fluctuations as an example. We calculate this observable for $K/pi$ fluctuations, and show that it should be the same for RHIC and LHC energies, as well as independent of centrality, if the Grand-Canonical statistical model is an appropriate description and chemical equilibrium applies. We also introduce a similar observable capable, together with the published $K*/K$ measurement, of ascertaining if an interacting hadron gas phase governs the system between thermal and\n"
          ]
        }
      ],
      "source": [
        "def show_rag_context_and_answer(query):\n",
        "    top_docs = search_faiss(query, top_k=3)\n",
        "    print(\"TÃ­tulos de documentos recuperados:\")\n",
        "    for i, doc in enumerate(top_docs):\n",
        "        print(f\"{i+1}. {doc['title']}\")\n",
        "    \n",
        "    print(\"\\nResumen combinado (contexto):\")\n",
        "    for doc in top_docs:\n",
        "        print(\"- \", doc['abstract'][:300], \"...\\n\")\n",
        "\n",
        "    respuesta = generate_rag_response(query)\n",
        "    print(\"\\n Respuesta generada por RAG:\\n\", respuesta)\n",
        "\n",
        "show_rag_context_and_answer(\"machine learning for particle physics\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Tabla comparativa (Benchmark) de resultados entre modelos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calidad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Query</th>\n",
              "      <th>TF-IDF - BM25</th>\n",
              "      <th>TF-IDF - FAISS</th>\n",
              "      <th>BM25 - FAISS</th>\n",
              "      <th>Todos comunes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>diphoton production cross sections</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>quantum chromodynamics</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>higgs boson decay</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>machine learning for particle physics</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>top quark production</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   Query  TF-IDF - BM25  TF-IDF - FAISS  \\\n",
              "0     diphoton production cross sections              9               4   \n",
              "1                 quantum chromodynamics              8               4   \n",
              "2                      higgs boson decay              8               2   \n",
              "3  machine learning for particle physics              3               0   \n",
              "4                   top quark production              5               5   \n",
              "\n",
              "   BM25 - FAISS  Todos comunes  \n",
              "0             3              3  \n",
              "1             4              4  \n",
              "2             4              2  \n",
              "3             1              0  \n",
              "4             5              3  "
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "# Leer queries desde el archivo txt\n",
        "with open(\"queries.txt\", \"r\") as f:\n",
        "    queries = [line.strip() for line in f.readlines() if line.strip()]\n",
        "\n",
        "# FunciÃ³n para evaluar calidad de recuperaciÃ³n\n",
        "def benchmark_models(queries):\n",
        "    records = []\n",
        "    for query in queries:\n",
        "        tfidf_ids = set([doc[\"id\"] for doc in search_tfidf(query)])\n",
        "        bm25_ids = set([doc[\"id\"] for doc in search_bm25(query)])\n",
        "        faiss_ids = set([doc[\"id\"] for doc in search_faiss(query)])\n",
        "\n",
        "        records.append({\n",
        "            \"Query\": query,\n",
        "            \"TF-IDF - BM25\": len(tfidf_ids & bm25_ids),\n",
        "            \"TF-IDF - FAISS\": len(tfidf_ids & faiss_ids),\n",
        "            \"BM25 - FAISS\": len(bm25_ids & faiss_ids),\n",
        "            \"Todos comunes\": len(tfidf_ids & bm25_ids & faiss_ids)\n",
        "        })\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "# Ejecutar el benchmark y mostrar la tabla\n",
        "df_benchmark = benchmark_models(queries)\n",
        "df_benchmark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La tabla nos dice quÃ© tan parecidos son los resultados que devuelve cada modelo.\n",
        "Vemos que TF-IDF y BM25 suelen coincidir bastante, porque ambos buscan las palabras tal como las escribiste.\n",
        "FAISS, en cambio, busca mÃ¡s por el sentido o el significado de lo que preguntas, asÃ­ que encuentra cosas distintas.\n",
        "Por eso, casi no hay documentos que aparezcan en los tres modelos a la vez. Cada uno entiende la bÃºsqueda a su manera.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ejemplo de una consulta y su respuesta generada con RAG\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Respuesta generada con RAG:\n",
            " A modified narrow-width approximation that allows for O(Gamma/M)-accurate predictions for resonant particle decay with similar intermediate masses is proposed and applied to MSSM processes to demonstrate its importance for searches for particle physics beyond the Standard Model. We explain how fluctuations of ratios can constrain and falsify the statistical model of particle production in heavy ion collisions, using $K/pi$ fluctuations as an example. We define an observable capable of determining which statistical model, if any, governs freeze-out in ultrarelativistic heavy ion collisions, using $K/pi$ fluctuations as an example. We calculate this observable for $K/pi$ fluctuations, and show that it should be the same for RHIC and LHC energies, as well as independent of centrality, if the Grand-Canonical statistical model is an appropriate description and chemical equilibrium applies. We also introduce a similar observable capable, together with the published $K*/K$ measurement, of ascertaining if an interacting hadron gas phase governs the system between thermal and\n"
          ]
        }
      ],
      "source": [
        "query = \"machine learning for particle physics\"\n",
        "respuesta = generate_rag_response(query)\n",
        "print(\"Respuesta generada con RAG:\\n\", respuesta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Diferencias entre modelos y utilidad del RAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Modelo</th>\n",
              "      <th>Ventajas</th>\n",
              "      <th>Limitaciones</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TF-IDF</td>\n",
              "      <td>RÃ¡pido, fÃ¡cil de implementar y eficiente en bÃº...</td>\n",
              "      <td>No capta el significado de las palabras; no de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BM25</td>\n",
              "      <td>Mejor manejo de la frecuencia de tÃ©rminos y mÃ¡...</td>\n",
              "      <td>Aunque mejora a TF-IDF, aÃºn es un modelo estad...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>FAISS</td>\n",
              "      <td>Captura similitud semÃ¡ntica entre frases o sin...</td>\n",
              "      <td>Necesita mayor poder computacional y puede rec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RAG (con Flan-T5)</td>\n",
              "      <td>Genera respuestas completas y justificadas usa...</td>\n",
              "      <td>Depende de la calidad del Ã­ndice vectorial y d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Modelo                                           Ventajas  \\\n",
              "0             TF-IDF  RÃ¡pido, fÃ¡cil de implementar y eficiente en bÃº...   \n",
              "1               BM25  Mejor manejo de la frecuencia de tÃ©rminos y mÃ¡...   \n",
              "2              FAISS  Captura similitud semÃ¡ntica entre frases o sin...   \n",
              "3  RAG (con Flan-T5)  Genera respuestas completas y justificadas usa...   \n",
              "\n",
              "                                        Limitaciones  \n",
              "0  No capta el significado de las palabras; no de...  \n",
              "1  Aunque mejora a TF-IDF, aÃºn es un modelo estad...  \n",
              "2  Necesita mayor poder computacional y puede rec...  \n",
              "3  Depende de la calidad del Ã­ndice vectorial y d...  "
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "comparacion_modelos = pd.DataFrame({\n",
        "    \"Modelo\": [\"TF-IDF\", \"BM25\", \"FAISS\", \"RAG (con Flan-T5)\"],\n",
        "    \"Ventajas\": [\n",
        "        \"Es rÃ¡pido, fÃ¡cil de programar y te devuelve buenos resultados si buscas palabras exactas.\",\n",
        "        \"Es mÃ¡s listo que TF-IDF porque da mÃ¡s peso a las palabras que realmente importan.\",\n",
        "        \"Captura similitud semÃ¡ntica entre frases o sinÃ³nimos gracias a embeddings.\",\n",
        "        \"Es como un asistente que busca y te explica. Resume la info y contesta con base en lo que encontrÃ³.\"\n",
        "    ],\n",
        "    \"Limitaciones\": [\n",
        "        \"No entiende el significado de las palabras. Si usas un sinÃ³nimo, no lo capta.\",\n",
        "        \"Aun asÃ­, sigue sin entender ideas, solo se enfoca en cuÃ¡ntas veces aparece algo.\",\n",
        "        \"Necesita mayor poder computacional y puede recuperar documentos no tan precisos.\",\n",
        "        \"Depende de la calidad del Ã­ndice vectorial y del modelo generativo usado.\"\n",
        "    ]\n",
        "})\n",
        "comparacion_modelos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utilidad del RAG\n",
        "\n",
        "Es Ãºtil cuando no se tiene tiempo (por ciertas circuntancias) para leer todo o cuando los textos son muy tÃ©cnicos. RAG da una respuesta clara, directa y con contexto, sin tener que estar adivinando quÃ© dice cada paper.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
